# Gemma Model Configuration
model_name: "google/gemma-2b"  # Can be gemma-2b or gemma-7b
max_length: 512
batch_size: 8

# Training parameters
epochs: 3
learning_rate: 2e-5
warmup_steps: 500
weight_decay: 0.01
gradient_accumulation_steps: 1

# Generation parameters
max_new_tokens: 100
temperature: 0.7
top_p: 0.9
do_sample: true

# Data parameters
data_dir: "data"
output_dir: "outputs"

# Hardware
use_fp16: true
device_map: "auto"