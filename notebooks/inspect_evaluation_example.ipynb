{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Gemma Model Evaluation with Inspect AI\n",
        "\n",
        "This notebook demonstrates how to use Inspect AI for comprehensive evaluation of Gemma models."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Installation\n",
        "\n",
        "First, install the required packages:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install Inspect AI and dependencies\n",
        "!pip install inspect-ai\n",
        "!pip install -r ../requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Basic Evaluation Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('..')\n",
        "\n",
        "from src.evaluation.inspect_evaluator import GemmaInspectEvaluator\n",
        "from src.models.gemma_model import GemmaModel\n",
        "import json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Initialize Evaluator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize evaluator with your model\n",
        "model_path = \"google/gemma-2b\"  # or path to your fine-tuned model\n",
        "evaluator = GemmaInspectEvaluator(model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Run Coding Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and run coding evaluation\n",
        "coding_task = evaluator.create_coding_eval()\n",
        "coding_results = evaluator.run_evaluation(\n",
        "    coding_task,\n",
        "    log_dir=\"./logs/coding\"\n",
        ")\n",
        "\n",
        "print(\"Coding Evaluation Results:\")\n",
        "print(f\"Accuracy: {coding_results['scores']['accuracy']:.2%}\")\n",
        "print(f\"Mean Score: {coding_results['scores']['mean_score']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Run Reasoning Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and run reasoning evaluation\n",
        "reasoning_task = evaluator.create_reasoning_eval()\n",
        "reasoning_results = evaluator.run_evaluation(\n",
        "    reasoning_task,\n",
        "    log_dir=\"./logs/reasoning\"\n",
        ")\n",
        "\n",
        "print(\"Reasoning Evaluation Results:\")\n",
        "print(f\"Accuracy: {reasoning_results['scores']['accuracy']:.2%}\")\n",
        "print(f\"Mean Score: {reasoning_results['scores']['mean_score']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Run Safety Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and run safety evaluation\n",
        "safety_task = evaluator.create_safety_eval()\n",
        "safety_results = evaluator.run_evaluation(\n",
        "    safety_task,\n",
        "    log_dir=\"./logs/safety\"\n",
        ")\n",
        "\n",
        "print(\"Safety Evaluation Results:\")\n",
        "print(f\"Accuracy: {safety_results['scores']['accuracy']:.2%}\")\n",
        "print(f\"Mean Score: {safety_results['scores']['mean_score']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Create Custom Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a custom evaluation for your specific use case\n",
        "custom_dataset = [\n",
        "    {\n",
        "        \"input\": \"Translate 'Hello world' to Spanish\",\n",
        "        \"target\": \"Hola mundo\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"What is the capital of France?\",\n",
        "        \"target\": \"Paris\"\n",
        "    },\n",
        "    {\n",
        "        \"input\": \"Calculate 15% of 200\",\n",
        "        \"target\": \"30\"\n",
        "    }\n",
        "]\n",
        "\n",
        "custom_task = evaluator.create_custom_eval(\n",
        "    name=\"custom_knowledge\",\n",
        "    dataset=custom_dataset,\n",
        "    system_prompt=\"You are a helpful assistant. Answer concisely and accurately.\",\n",
        "    scoring_method=\"includes\"\n",
        ")\n",
        "\n",
        "custom_results = evaluator.run_evaluation(\n",
        "    custom_task,\n",
        "    log_dir=\"./logs/custom\"\n",
        ")\n",
        "\n",
        "print(\"Custom Evaluation Results:\")\n",
        "print(f\"Accuracy: {custom_results['scores']['accuracy']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Run Comprehensive Evaluation Suite"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from src.evaluation.inspect_evaluator import run_comprehensive_evaluation\n",
        "\n",
        "# Run full evaluation suite\n",
        "comprehensive_results = run_comprehensive_evaluation(\n",
        "    model_path=model_path,\n",
        "    output_dir=\"./evaluation_results\"\n",
        ")\n",
        "\n",
        "print(\"\\nComprehensive Evaluation Summary:\")\n",
        "print(f\"Overall Score: {comprehensive_results['overall_score']:.2%}\")\n",
        "print(\"\\nDetailed Results:\")\n",
        "for eval_name, results in comprehensive_results['evaluations'].items():\n",
        "    print(f\"\\n{eval_name.capitalize()}:\")\n",
        "    print(f\"  - Accuracy: {results['scores']['accuracy']:.2%}\")\n",
        "    print(f\"  - Duration: {results['duration']:.2f}s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Compare Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare base model vs fine-tuned model\n",
        "models_to_compare = [\n",
        "    \"google/gemma-2b\",\n",
        "    \"./outputs/checkpoints/final\"  # Your fine-tuned model\n",
        "]\n",
        "\n",
        "comparison_results = {}\n",
        "\n",
        "for model in models_to_compare:\n",
        "    print(f\"\\nEvaluating {model}...\")\n",
        "    results = run_comprehensive_evaluation(\n",
        "        model_path=model,\n",
        "        output_dir=f\"./comparison/{model.replace('/', '_')}\"\n",
        "    )\n",
        "    comparison_results[model] = results\n",
        "\n",
        "# Display comparison\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"Model Comparison Results\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "for model, results in comparison_results.items():\n",
        "    print(f\"\\nModel: {model}\")\n",
        "    print(f\"Overall Score: {results['overall_score']:.2%}\")\n",
        "    for eval_name, eval_results in results['evaluations'].items():\n",
        "        print(f\"  {eval_name}: {eval_results['scores']['accuracy']:.2%}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Export Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save comprehensive results to file\n",
        "with open('evaluation_report.json', 'w') as f:\n",
        "    json.dump(comprehensive_results, f, indent=2)\n",
        "\n",
        "print(\"Results saved to evaluation_report.json\")\n",
        "\n",
        "# Generate markdown report\n",
        "report = f\"\"\"# Gemma Model Evaluation Report\n",
        "\n",
        "## Model: {comprehensive_results['model']}\n",
        "\n",
        "### Overall Score: {comprehensive_results['overall_score']:.2%}\n",
        "\n",
        "### Detailed Results:\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "for eval_name, results in comprehensive_results['evaluations'].items():\n",
        "    report += f\"\\n#### {eval_name.capitalize()} Evaluation\\n\"\n",
        "    report += f\"- Accuracy: {results['scores']['accuracy']:.2%}\\n\"\n",
        "    report += f\"- Mean Score: {results['scores']['mean_score']:.3f}\\n\"\n",
        "    report += f\"- Std Dev: {results['scores']['std_score']:.3f}\\n\"\n",
        "    report += f\"- Duration: {results['duration']:.2f}s\\n\"\n",
        "\n",
        "with open('evaluation_report.md', 'w') as f:\n",
        "    f.write(report)\n",
        "\n",
        "print(\"Markdown report saved to evaluation_report.md\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Extract scores for visualization\n",
        "eval_names = list(comprehensive_results['evaluations'].keys())\n",
        "scores = [results['scores']['accuracy'] for results in comprehensive_results['evaluations'].values()]\n",
        "\n",
        "# Create bar chart\n",
        "plt.figure(figsize=(10, 6))\n",
        "bars = plt.bar(eval_names, scores)\n",
        "\n",
        "# Customize colors\n",
        "colors = ['#4CAF50', '#2196F3', '#FF9800']\n",
        "for bar, color in zip(bars, colors):\n",
        "    bar.set_color(color)\n",
        "\n",
        "plt.title(f'Gemma Model Evaluation Results\\nOverall Score: {comprehensive_results[\"overall_score\"]:.2%}')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.ylim(0, 1)\n",
        "\n",
        "# Add value labels on bars\n",
        "for i, (name, score) in enumerate(zip(eval_names, scores)):\n",
        "    plt.text(i, score + 0.02, f'{score:.1%}', ha='center')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('evaluation_results.png', dpi=300)\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}